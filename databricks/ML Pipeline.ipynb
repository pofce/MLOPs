{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d48dc04-8f5d-4aa4-8b3e-fba3655eeda3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting transformers\n  Downloading transformers-4.42.4-py3-none-any.whl (9.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.3/9.3 MB 16.7 MB/s eta 0:00:00\nCollecting torch\n  Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 779.1/779.1 MB 899.9 kB/s eta 0:00:00\nRequirement already satisfied: mlflow in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (2.14.3)\nCollecting tokenizers<0.20,>=0.19\n  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 40.1 MB/s eta 0:00:00\nCollecting safetensors>=0.4.1\n  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 22.5 MB/s eta 0:00:00\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\nCollecting regex!=2019.12.17\n  Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 775.1/775.1 kB 49.2 MB/s eta 0:00:00\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.10/site-packages (from transformers) (23.2)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.10/site-packages (from transformers) (2.28.1)\nCollecting tqdm>=4.27\n  Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.3/78.3 kB 15.4 MB/s eta 0:00:00\nRequirement already satisfied: pyyaml>=5.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: numpy<2.0,>=1.17 in /databricks/python3/lib/python3.10/site-packages (from transformers) (1.23.5)\nCollecting huggingface-hub<1.0,>=0.23.2\n  Downloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 402.6/402.6 kB 42.6 MB/s eta 0:00:00\nCollecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.2/124.2 MB 11.7 MB/s eta 0:00:00\nRequirement already satisfied: typing-extensions>=4.8.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from torch) (4.12.2)\nCollecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 99.1/99.1 kB 16.5 MB/s eta 0:00:00\nCollecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.6/823.6 kB 51.0 MB/s eta 0:00:00\nCollecting nvidia-cudnn-cu12==8.9.2.26\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 731.7/731.7 MB 887.8 kB/s eta 0:00:00\nCollecting sympy\n  Downloading sympy-1.13.0-py3-none-any.whl (6.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/6.2 MB 82.8 MB/s eta 0:00:00\nCollecting fsspec\n  Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.6/177.6 kB 28.3 MB/s eta 0:00:00\nCollecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 62.0 MB/s eta 0:00:00\nCollecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 MB 1.5 MB/s eta 0:00:00\nCollecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.0/196.0 MB 6.4 MB/s eta 0:00:00\nCollecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 176.2/176.2 MB 5.6 MB/s eta 0:00:00\nCollecting triton==2.3.1\n  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 168.1/168.1 MB 6.2 MB/s eta 0:00:00\nCollecting networkx\n  Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 22.2 MB/s eta 0:00:00\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/23.7 MB 45.9 MB/s eta 0:00:00\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.10/site-packages (from torch) (3.1.2)\nCollecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.5/56.5 MB 24.9 MB/s eta 0:00:00\nCollecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.6/121.6 MB 10.6 MB/s eta 0:00:00\nCollecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.3/21.3 MB 57.0 MB/s eta 0:00:00\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (3.7.0)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (1.10.0)\nRequirement already satisfied: click<9,>=7.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from mlflow) (8.1.7)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from mlflow) (3.1.43)\nRequirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from mlflow) (7.1.0)\nRequirement already satisfied: alembic!=1.10.0,<2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from mlflow) (1.13.2)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from mlflow) (1.25.0)\nRequirement already satisfied: pytz<2025 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (2022.7)\nRequirement already satisfied: Flask<4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from mlflow) (3.0.3)\nRequirement already satisfied: querystring-parser<2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from mlflow) (1.2.4)\nRequirement already satisfied: cachetools<6,>=5.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from mlflow) (5.3.3)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from mlflow) (2.0.31)\nRequirement already satisfied: pyarrow<16,>=4.0.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (8.0.0)\nRequirement already satisfied: graphene<4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from mlflow) (3.3)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (1.1.1)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (4.24.0)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from mlflow) (1.25.0)\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (1.5.3)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from mlflow) (0.5.0)\nRequirement already satisfied: entrypoints<1 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (0.4)\nRequirement already satisfied: gunicorn<23 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from mlflow) (22.0.0)\nRequirement already satisfied: markdown<4,>=3.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from mlflow) (3.6)\nRequirement already satisfied: cloudpickle<4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from mlflow) (3.0.0)\nRequirement already satisfied: docker<8,>=4.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from mlflow) (7.1.0)\nRequirement already satisfied: Mako in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.5)\nRequirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.10/site-packages (from docker<8,>=4.0.0->mlflow) (1.26.14)\nRequirement already satisfied: itsdangerous>=2.1.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from Flask<4->mlflow) (2.2.0)\nRequirement already satisfied: blinker>=1.6.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from Flask<4->mlflow) (1.8.2)\nRequirement already satisfied: Werkzeug>=3.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from Flask<4->mlflow) (3.0.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from gitpython<4,>=3.1.9->mlflow) (4.0.11)\nRequirement already satisfied: aniso8601<10,>=8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from graphene<4->mlflow) (9.0.1)\nRequirement already satisfied: graphql-relay<3.3,>=3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from graphene<4->mlflow) (3.2.0)\nRequirement already satisfied: graphql-core<3.3,>=3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from graphene<4->mlflow) (3.2.3)\nRequirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow) (1.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.0.5)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.4.4)\nRequirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (3.0.9)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (9.4.0)\nRequirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (2.8.2)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (0.11.0)\nRequirement already satisfied: deprecated>=1.2.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow) (1.2.14)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow) (0.46b0)\nRequirement already satisfied: six in /usr/lib/python3/dist-packages (from querystring-parser<2->mlflow) (1.16.0)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (2.2.0)\nRequirement already satisfied: joblib>=1.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (1.2.0)\nRequirement already satisfied: greenlet!=0.4.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.3)\nCollecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 536.2/536.2 kB 55.8 MB/s eta 0:00:00\nRequirement already satisfied: wrapt<2,>=1.10 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow) (1.16.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-31150ca4-d6b5-4669-98a4-354d5bb82284/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow) (5.0.1)\nInstalling collected packages: mpmath, triton, tqdm, sympy, safetensors, regex, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, torch\nSuccessfully installed fsspec-2024.6.1 huggingface-hub-0.23.4 mpmath-1.3.0 networkx-3.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 regex-2024.5.15 safetensors-0.4.3 sympy-1.13.0 tokenizers-0.19.1 torch-2.3.1 tqdm-4.66.4 transformers-4.42.4 triton-2.3.1\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch mlflow\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "794e5469-5636-443b-8592-74054e318153",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import mlflow\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa897747-b075-4b71-805d-be2e528aa437",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### S3 Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2210f1f9-0b5e-4461-a14f-b698e70063d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_s3_data_to_dataframe(s3_client, bucket_name, file_key):\n",
    "    \"\"\"\n",
    "    Fetches and converts a single CSV file from an S3 bucket into a DataFrame.\n",
    "    \"\"\"\n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    response_body = response['Body'].read()\n",
    "    data = pd.read_csv(BytesIO(response_body))\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_recent_csvs_from_s3(s3_client, bucket_name, prefix):\n",
    "    \"\"\"\n",
    "    Retrieves the two most recent CSV files from a specified folder in an S3 bucket and merges them into one DataFrame.\n",
    "    \"\"\"\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    \n",
    "    files = sorted(\n",
    "        (item for item in response.get('Contents', []) if item['Key'].endswith('.csv')),\n",
    "        key=lambda item: item['LastModified'],\n",
    "        reverse=True\n",
    "    )\n",
    "    data_frames = [\n",
    "        pd.read_csv(BytesIO(s3_client.get_object(Bucket=bucket_name, Key=file['Key'])['Body'].read()))\n",
    "         for file in files[:2]\n",
    "    ]\n",
    "    combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "\n",
    "aws_access_key_id = dbutils.secrets.get(scope=\"mlops\", key=\"aws_access_key_id\")\n",
    "aws_secret_access_key = dbutils.secrets.get(scope=\"mlops\", key=\"aws_secret_access_key\")\n",
    "\n",
    "\n",
    "client = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n",
    "bucket_name = 'pofce-mlops-bucket'\n",
    "final_val_data = load_s3_data_to_dataframe(client, bucket_name, 'val_data.csv')\n",
    "data = load_recent_csvs_from_s3(client, bucket_name, 'train_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe8afaa7-91fa-4a9e-b44e-2fc3325d58d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# data = data.iloc[:750]\n",
    "# final_val_data = final_val_data.iloc[:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dd6eb08-32cf-4ce6-b4fd-b4555d758244",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Train_data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2c57c8d-ca0b-4fff-9880-eed095d851be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "data.dropna(inplace=True)\n",
    "data.drop_duplicates(subset=['clean_text'], inplace=True)\n",
    "\n",
    "texts = data['clean_text'].values\n",
    "labels = data['category'].values\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels_train = label_encoder.fit_transform(train_labels)\n",
    "label_mapping = {original_label: int_label for original_label, int_label in zip(train_labels, encoded_labels_train)}\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels_valid = label_encoder.fit_transform(val_labels)\n",
    "label_mapping = {original_label: int_label for original_label, int_label in zip(val_labels, encoded_labels_valid)}\n",
    "\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)\n",
    "\n",
    "\n",
    "class Sentiment(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "train_labels = encoded_labels_train\n",
    "val_labels = encoded_labels_valid\n",
    "\n",
    "train_data = Sentiment(train_texts, train_labels, tokenizer, max_len=128)\n",
    "val_data = Sentiment(val_texts, val_labels, tokenizer, max_len=128)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "953993df-b6e7-412d-894b-2db265a90206",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Evaluation of Model Performance on Test Data\n",
    "This code cell is designed to evaluate a trained model's performance on a validation dataset by calculating metrics such as accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a2c820b-2c30-497b-a25e-9a50dd8e8d24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_test_scores(device):\n",
    "    final_val_data.rename(columns={'clean_comment': 'clean_text'}, inplace=True)\n",
    "\n",
    "    final_val_data.dropna(inplace=True)\n",
    "\n",
    "    final_val_data.drop_duplicates(subset=['clean_text'], inplace=True)\n",
    "\n",
    "    final_val_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    test_enc_labels = label_encoder.transform(final_val_data['category'])\n",
    "    test_dataset = Sentiment(final_val_data['clean_text'], test_enc_labels, tokenizer, max_len=128)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    predicted_labels = label_encoder.inverse_transform(all_predictions)\n",
    "    true_labels = label_encoder.inverse_transform(all_true_labels)\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    report_dict = classification_report(true_labels, predicted_labels, output_dict=True)\n",
    "    return accuracy, report_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91f4dfda-ee9f-4bfe-ad9b-f93d0ba58611",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Setting Up MLflow Model Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d17442f1-0ba1-46fb-a519-4640ee7ecff5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "\n",
    "input_schema = Schema([ColSpec(\"long\", \"input_ids\"), ColSpec(\"long\", \"attention_mask\"), ColSpec(\"long\", \"labels\")])\n",
    "output_schema = Schema([ColSpec(\"float\", \"logits\")])\n",
    "\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2949cc1c-4499-4558-b5e4-e328a6f365d5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46188d7d-ffc2-46be-905c-87e89a0d22af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca439b6b4564899b37e2803b5cc9848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Registered model 'MyModel' already exists. Creating a new version of this model...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f90eea1e89b4161843088fa233d44cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ca20cbd08543d195b7d840715f6bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Created version '2' of model 'mlops.default.mymodel'.\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader):\n",
    "    with mlflow.start_run(run_name=\"MLOPs Tracking Enhanced\"):\n",
    "        # Define hyperparameters\n",
    "        params = {\n",
    "            \"learning_rate\": 1e-5,\n",
    "            \"epochs\": 1,\n",
    "            \"optimizer\": \"AdamW\",\n",
    "            \"batch_size\": len(train_loader.batch_sampler),\n",
    "            \"scheduler\": \"StepLR\"\n",
    "        }\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        # Setup optimizer and scheduler\n",
    "        optimizer = AdamW(model.parameters(), lr=params[\"learning_rate\"])\n",
    "        scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(params[\"epochs\"]):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            for batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss  # Assuming the model returns a named tuple with loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            scheduler.step()\n",
    "            mlflow.log_metric(\"loss\", epoch_loss / len(train_loader), step=epoch)\n",
    "\n",
    "            # Validation loop\n",
    "            accuracy, report_dict = get_test_scores(device)\n",
    "            mlflow.log_metric(\"accuracy\", accuracy)\n",
    "            mlflow.log_metric(\"macro_avg_precision\", report_dict[\"macro avg\"][\"precision\"])\n",
    "            mlflow.log_metric(\"macro_avg_recall\", report_dict[\"macro avg\"][\"recall\"])\n",
    "            mlflow.log_metric(\"macro_avg_f1-score\", report_dict[\"macro avg\"][\"f1-score\"])\n",
    "\n",
    "        # Log and register model with a signature\n",
    "        mlflow.pytorch.log_model(model, \"model\", signature=signature)\n",
    "        model_name = \"MyModel\"\n",
    "        model_uri = f\"runs:/{mlflow.active_run().info.run_id}/model\"\n",
    "        model_info = mlflow.register_model(model_uri, model_name)\n",
    "\n",
    "        return model_info\n",
    "\n",
    "\n",
    "challenger_info = train_model(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28cb2b09-0730-41fa-91bd-845a7e5b6d83",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Model Version Management with MLflow\n",
    "\n",
    "This process ensures that the most accurate model is always designated as the \"Champion,\" fostering a continuous improvement cycle in model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e0945db-b0b2-4d18-b7cf-62ad6cc15593",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "if challenger_info.version == '1':\n",
    "    client.set_registered_model_alias(challenger_info.name, \"Champion\", challenger_info.version)\n",
    "else:\n",
    "    champion_version = client.get_model_version_by_alias(challenger_info.name, \"Champion\")\n",
    "    champion_accuracy = client.get_metric_history(champion_version.run_id, \"accuracy\")[0].value\n",
    "    challenger_accuracy = client.get_metric_history(challenger_info.run_id, \"accuracy\")[0].value\n",
    "\n",
    "    if champion_accuracy < challenger_accuracy:\n",
    "        client.set_registered_model_alias(champion_version.name, \"Retired\", champion_version.version)\n",
    "        client.set_registered_model_alias(challenger_info.name, \"Champion\", challenger_info.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5f42de0-967f-4a8e-a087-bed64557b4a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Automatic Deployment\n",
    "\n",
    "This feature has been disabled because it is not available in the trial version. Instead, we are using a local API. However, this feature will be fully functional in the complete workspace environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb4a1b43-c4de-44bd-b18e-a8110fb3952b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n    \"error_code\": \"FEATURE_DISABLED\",\n    \"message\": \"Model serving is not available for trial workspaces. Please contact your organization admin or Databricks support.\"\n}\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import requests\n",
    "\n",
    "# champion_info = client.get_model_version_by_alias(\"mymodel\", \"Champion\")\n",
    "\n",
    "# API_ROOT = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get() \n",
    "# API_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "# data = {\n",
    "#     \"name\": \"SentimentService\",\n",
    "#     \"config\": {\n",
    "#         \"served_entities\": [\n",
    "#             {\n",
    "#                 \"entity_name\": champion_info.name,\n",
    "#                 \"entity_version\": champion_info.version,\n",
    "#                 \"workload_size\": \"Small\",\n",
    "#                 \"scale_to_zero_enabled\": False,\n",
    "#                 \"workload_type\": \"CPU\",\n",
    "#             }\n",
    "#         ]\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# headers = {\"Context-Type\": \"text/json\", \"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "# response = requests.post(\n",
    "#     url=f\"{API_ROOT}/api/2.0/serving-endpoints\", json=data, headers=headers\n",
    "# )\n",
    "\n",
    "# print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "# {\n",
    "#     \"error_code\": \"FEATURE_DISABLED\",\n",
    "#     \"message\": \"Model serving is not available for trial workspaces. Please contact your organization admin or Databricks support.\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b578e520-cc5a-4639-b1e7-52f6f14e8f01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ML Pipeline",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
